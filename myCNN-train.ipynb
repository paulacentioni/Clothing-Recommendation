{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los archivos de configuración y pesos\n",
    "config_path = 'conf.cfg'\n",
    "weights_path = 'pesos.weights'\n",
    "names_path = 'obj.names'\n",
    "\n",
    "# Cargar la lista de nombres de clases\n",
    "with open(names_path, 'r') as f:\n",
    "    class_names = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Cargar el modelo YOLOv4\n",
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "\n",
    "# Obtener la lista de capas de salida\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "image_files = []\n",
    "image_list = []\n",
    "\n",
    "# Obtener la lista de archivos de imagen en la carpeta \"fotos\"\n",
    "# Abajo se define el path que contiene varios directorios con imagenes.\n",
    "path = './ComputerVision2'\n",
    "for root, dirs, files in os.walk(path):\n",
    "    print(root, dirs, files)\n",
    "    for name in files:\n",
    "        image_list = glob.glob(root + '/*.jpg') # Creamos lista de imagenes por directorio\n",
    "    image_files.extend(image_list) # Extendemos la lista que contiene las imagenes de todos los directorios \n",
    "\n",
    "img_index = 0 # Indice para utilizar en la escritura de las imagenes \n",
    "# Iterar sobre cada imagen\n",
    "for image_path in image_files:\n",
    "    if 'ROI' not in image_path:\n",
    "        # Cargar la imagen de entrada\n",
    "        image = cv2.imread(image_path)\n",
    "        height, width, channels = image.shape\n",
    "\n",
    "        # Preprocesar la imagen para que se ajuste a la entrada del modelo\n",
    "        blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "\n",
    "        # Establecer la entrada del modelo\n",
    "        net.setInput(blob)\n",
    "\n",
    "        # Ejecutar la detección de objetos\n",
    "        layer_outputs = net.forward(output_layers)\n",
    "\n",
    "        # Recopilar información sobre detecciones de personas\n",
    "        boxes = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "\n",
    "        for output in layer_outputs:\n",
    "            for detection in output:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5 and class_id == 0:  # Clase 0 es para personas\n",
    "                    # Escalar las coordenadas de la caja delimitadora al tamaño de la imagen original\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "\n",
    "                    # Calcular las coordenadas de la esquina superior izquierda de la caja delimitadora\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Aplicar supresión no máxima para eliminar detecciones superpuestas\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        # Encontrar el bounding box de mayor área\n",
    "        max_area = 0\n",
    "        max_box = None\n",
    "\n",
    "        for i in indices:\n",
    "            x, y, w, h = boxes[i]\n",
    "            area = w * h\n",
    "\n",
    "            if area > max_area:\n",
    "                max_area = area\n",
    "                max_box = (x, y, w, h)\n",
    "                max_label = class_names[class_ids[i]]\n",
    "                max_confidence = confidences[i]\n",
    "\n",
    "        # Verificar si se encontró una persona\n",
    "        if max_box is not None:\n",
    "            x, y, w, h = max_box\n",
    "            label = max_label\n",
    "            confidence = max_confidence\n",
    "\n",
    "            # Valido que el bounding box no se salga de la img\n",
    "            if y < 0:\n",
    "                y = 1\n",
    "            if x < 0:\n",
    "                x = 1\n",
    "            \n",
    "            # Extraer la ROI\n",
    "            roi = image[y:y+h, x:x+w]\n",
    "\n",
    "            if all(roi.shape): # Valido que todos las componentes de la forma de la ROI sean > 0\n",
    "\n",
    "                output_path = image_path.split(\"\\\\\")[:-1][0] # Recorto el path con la \\\\ y me quedo con la primer parte para reutilizar\n",
    "            \n",
    "                cv2.imwrite(f'{output_path}/ROI_{label}_{confidence:.2f}_{img_index}.jpg', roi) # Guardar la ROI en un archivo\n",
    "        else:\n",
    "            # No se encontró ninguna persona, pasar a la siguiente imagen\n",
    "            continue\n",
    "        \n",
    "        # Se incrementa el indice para el nombre de la imagen\n",
    "        img_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "folder_path = r'./fotos'  # Ruta de la carpeta con las fotos\n",
    "extension = '.jpg'  # Extensión de los archivos de imagen\n",
    "\n",
    "counter = 1  # Contador para el número secuencial\n",
    "\n",
    "# Recorrer los archivos en la carpeta\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(extension):\n",
    "        # Obtener la ruta completa del archivo actual y el nuevo archivo\n",
    "        current_path = os.path.join(folder_path, filename)\n",
    "        new_filename = '0' + str(counter) + extension\n",
    "        new_path = os.path.join(folder_path, new_filename)\n",
    "\n",
    "        # Renombrar el archivo\n",
    "        os.rename(current_path, new_path)\n",
    "\n",
    "        counter += 1  # Incrementar el contador para el siguiente archivo\n",
    "\n",
    "print('Renombrado completado.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "import torchvision\n",
    "import pathlib\n",
    "import glob\n",
    "\n",
    "#checking for device\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "\n",
    "\n",
    "#Transforms\n",
    "transformer=transforms.Compose([\n",
    "    transforms.Resize((150,150)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),  #0-255 to 0-1, numpy to tensors\n",
    "    transforms.Normalize([0.5,0.5,0.5], # 0-1 to [-1,1] , formula (x-mean)/std\n",
    "                        [0.5,0.5,0.5])\n",
    "])\n",
    "\n",
    "\n",
    "#Path for training and testing directory\n",
    "train_path=r'.\\TRAIN-RECORTAR'\n",
    "test_path=r'.\\TEST-RECORTAR'\n",
    "torch.manual_seed(42)\n",
    "\n",
    "train_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(train_path,transform=transformer),\n",
    "    batch_size=32, shuffle=True\n",
    ")\n",
    "test_loader=DataLoader(\n",
    "    torchvision.datasets.ImageFolder(test_path,transform=transformer),\n",
    "    batch_size=32, shuffle=True\n",
    ")\n",
    "\n",
    "#categories\n",
    "root=pathlib.Path(train_path)\n",
    "classes=sorted([j.name.split('/')[-1] for j in root.iterdir()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self,num_classes=5):\n",
    "        super(ConvNet,self).__init__()\n",
    "\n",
    "        self.conv1=nn.Conv2d(in_channels=3,out_channels=12,kernel_size=3,stride=1,padding=1)\n",
    "        self.bn1=nn.BatchNorm2d(num_features=12)\n",
    "\n",
    "        self.relu1=nn.ReLU()\n",
    "\n",
    "        \n",
    "        self.pool=nn.MaxPool2d(kernel_size=2)  \n",
    "        \n",
    "        self.conv2=nn.Conv2d(in_channels=12,out_channels=20,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.relu2=nn.ReLU()\n",
    "        \n",
    "        self.conv3=nn.Conv2d(in_channels=20,out_channels=32,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "        self.bn3=nn.BatchNorm2d(num_features=32)\n",
    "\n",
    "        self.relu3=nn.ReLU()\n",
    "        \n",
    "        self.fc=nn.Linear(in_features=75 * 75 * 32,out_features=num_classes)\n",
    "        \n",
    "    def forward(self,input):\n",
    "        output=self.conv1(input)\n",
    "        output=self.bn1(output)\n",
    "        output=self.relu1(output)\n",
    "            \n",
    "        output=self.pool(output)\n",
    "            \n",
    "        output=self.conv2(output)\n",
    "        output=self.relu2(output)\n",
    "            \n",
    "        output=self.conv3(output)\n",
    "        output=self.bn3(output)\n",
    "        output=self.relu3(output)\n",
    "            \n",
    "        output=output.view(-1,32*75*75)\n",
    "            \n",
    "            \n",
    "        output=self.fc(output)\n",
    "            \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ConvNet(num_classes=5).to(device)\n",
    "optimizer=Adam(model.parameters(),lr=0.001,weight_decay=0.0001)\n",
    "loss_function=nn.CrossEntropyLoss()\n",
    "\n",
    "train_count=len(glob.glob(train_path+'/**/*.jpg'))\n",
    "test_count=len(glob.glob(test_path+'/**/*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_epochs = 10\n",
    "best_accuracy = 0.0\n",
    "\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    train_count = 0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.cpu().data * images.size(0)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        train_accuracy += int(torch.sum(prediction == labels.data))\n",
    "        train_count += labels.size(0)\n",
    "\n",
    "    train_loss = train_loss / train_count\n",
    "    train_accuracy = train_accuracy / train_count\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    \n",
    "    model.eval()\n",
    "    test_accuracy = 0.0\n",
    "    test_count = 0\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    for i, (images, labels) in enumerate(test_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            images = Variable(images.cuda())\n",
    "            labels = Variable(labels.cuda())\n",
    "\n",
    "        outputs = model(images)\n",
    "        _, prediction = torch.max(outputs.data, 1)\n",
    "        test_accuracy += int(torch.sum(prediction == labels.data))\n",
    "        test_count += labels.size(0)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        test_loss += loss.cpu().data * images.size(0)\n",
    "    \n",
    "    test_loss = test_loss / test_count\n",
    "    test_accuracy = test_accuracy / test_count\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    #print('Epoch: ' + str(epoch) + ' Train Loss: ' + str(train_loss) + ' Train Accuracy: ' + str(train_accuracy) + ' Test Accuracy: ' + str(test_accuracy))\n",
    "\n",
    "    if test_accuracy > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_checkpoint.model')\n",
    "        best_accuracy = test_accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
